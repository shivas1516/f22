{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faster-whisper gradio torchaudio soundfile resampy"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-09T04:56:10.968187Z",
          "iopub.execute_input": "2025-06-09T04:56:10.969145Z",
          "iopub.status.idle": "2025-06-09T04:56:14.872294Z",
          "shell.execute_reply.started": "2025-06-09T04:56:10.969104Z",
          "shell.execute_reply": "2025-06-09T04:56:14.871248Z"
        },
        "id": "AYRIXXk7Qx5k",
        "outputId": "eff17776-c762-400f-ef6a-35d7b3aaf23a",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faster-whisper\n",
            "  Downloading faster_whisper-1.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.47.2)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Collecting resampy\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
            "  Downloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.35.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (0.22.1)\n",
            "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
            "  Downloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting av>=11 (from faster-whisper)\n",
            "  Downloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from faster-whisper) (4.67.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.19.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.37.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from torchaudio) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0->torchaudio) (3.4.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
            "Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.12/dist-packages (from resampy) (0.60.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.13->faster-whisper) (1.1.10)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.53->resampy) (0.43.0)\n",
            "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.9.23)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0->torchaudio) (1.3.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading faster_whisper-1.2.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-15.1.0-cp312-cp312-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ctranslate2-4.6.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.8/38.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.23.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, ctranslate2, av, resampy, coloredlogs, onnxruntime, faster-whisper\n",
            "Successfully installed av-15.1.0 coloredlogs-15.0.1 ctranslate2-4.6.0 faster-whisper-1.2.0 humanfriendly-10.0 onnxruntime-1.23.0 resampy-0.4.3\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import soundfile as sf\n",
        "import resampy\n",
        "import subprocess\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "compute_type = \"float16\" if device == \"cuda\" else \"float32\"\n",
        "model_size = \"large-v2\"\n",
        "model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
        "\n",
        "def vad_transcribe(audio_file, threshold = 0.5, min_speech_ms = 250, max_speech_s = 20, min_silence_ms = 200):\n",
        "    try:\n",
        "        if audio_file is None:\n",
        "            return \"[Error] No file provided.\", None, None, None, None\n",
        "\n",
        "        audio_data, sr = sf.read(audio_file)\n",
        "        if len(audio_data.shape) > 1:\n",
        "            audio_data = np.mean(audio_data, axis=1)\n",
        "        audio_16k = resampy.resample(audio_data, sr, 16000).astype(np.float32)\n",
        "\n",
        "        total_duration = len(audio_16k) / 16000.0\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        segments, _ = model.transcribe(\n",
        "            audio_16k,\n",
        "            vad_filter=True,\n",
        "            vad_parameters={\n",
        "                \"threshold\": threshold,\n",
        "                \"min_speech_duration_ms\": min_speech_ms,\n",
        "                \"max_speech_duration_s\": max_speech_s,\n",
        "                \"min_silence_duration_ms\": min_silence_ms\n",
        "            }\n",
        "        )\n",
        "\n",
        "        end_time = time.time()\n",
        "        latency = round(end_time - start_time, 2)\n",
        "\n",
        "        transcript = \"\"\n",
        "        speech_duration = 0\n",
        "        segment_count = 0\n",
        "\n",
        "        for segment in segments:\n",
        "            transcript += f\"[{segment.start:.2f}s - {segment.end:.2f}s]: {segment.text.strip()}\\n\"\n",
        "            speech_duration += (segment.end - segment.start)\n",
        "            segment_count += 1\n",
        "\n",
        "        speech_ratio = round((speech_duration / total_duration) * 100, 2) if total_duration else 0\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            subprocess.run([\"nvidia-smi\"])\n",
        "\n",
        "        return (\n",
        "            transcript.strip(),\n",
        "            f\"{latency} seconds\"\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"[Error] {str(e)}\", None\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🎤 Voice Activity Detection + Transcription using Faster-Whisper + Gradio\")\n",
        "    gr.Markdown(\"Upload an audio file and apply configurable Voice Activity Detection.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            audio_input = gr.Audio(label=\"Upload Audio File\", type=\"filepath\")\n",
        "            transcribe_btn = gr.Button(\"Transcribe with VAD\")\n",
        "\n",
        "    with gr.Row():\n",
        "        transcript_output = gr.Textbox(label=\"📋 Transcription with Timestamps\", lines=12)\n",
        "\n",
        "    with gr.Row():\n",
        "        latency_output = gr.Textbox(label=\"⏱️ Latency\")\n",
        "\n",
        "    transcribe_btn.click(\n",
        "        fn=vad_transcribe,\n",
        "        inputs=[audio_input],\n",
        "        outputs=[transcript_output, latency_output]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-09T04:56:16.433411Z",
          "iopub.execute_input": "2025-06-09T04:56:16.434345Z",
          "iopub.status.idle": "2025-06-09T04:56:49.442038Z",
          "shell.execute_reply.started": "2025-06-09T04:56:16.434298Z",
          "shell.execute_reply": "2025-06-09T04:56:49.441125Z"
        },
        "id": "qmDm001TQx5m",
        "outputId": "67b377dc-3f22-4f73-8770-d5c52d9549d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f7b82c46f5259acb3d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f7b82c46f5259acb3d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import torch\n",
        "import subprocess\n",
        "import gradio as gr\n",
        "\n",
        "# 🔐 Set your AssemblyAI API Key here\n",
        "ASSEMBLYAI_API_KEY = \"af4b5d85f2214bc5a356a262b9ce6e21\"\n",
        "\n",
        "# 🔁 Upload file to AssemblyAI\n",
        "def upload_to_assemblyai(file_path):\n",
        "    headers = {'authorization': ASSEMBLYAI_API_KEY}\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = requests.post(\n",
        "            'https://api.assemblyai.com/v2/upload',\n",
        "            headers=headers,\n",
        "            files={'file': f}\n",
        "        )\n",
        "    return response.json()['upload_url']\n",
        "\n",
        "# 🧠 Transcribe and diarize\n",
        "def transcribe_and_diarize(audio_path):\n",
        "    try:\n",
        "        if not audio_path:\n",
        "            return \"[ERROR] No audio provided.\", \"\", \"\", \"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Upload\n",
        "        upload_url = upload_to_assemblyai(audio_path)\n",
        "\n",
        "        # Request transcription with speaker diarization\n",
        "        transcript_request = {\n",
        "            \"audio_url\": upload_url,\n",
        "            \"speaker_labels\": True,\n",
        "        }\n",
        "\n",
        "        headers = {\n",
        "            \"authorization\": ASSEMBLYAI_API_KEY,\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        response = requests.post(\n",
        "            \"https://api.assemblyai.com/v2/transcript\",\n",
        "            json=transcript_request,\n",
        "            headers=headers\n",
        "        )\n",
        "\n",
        "        transcript_id = response.json()['id']\n",
        "\n",
        "        # Polling for completion\n",
        "        polling_url = f\"https://api.assemblyai.com/v2/transcript/{transcript_id}\"\n",
        "        while True:\n",
        "            polling_response = requests.get(polling_url, headers=headers).json()\n",
        "            if polling_response['status'] == 'completed':\n",
        "                break\n",
        "            elif polling_response['status'] == 'error':\n",
        "                raise Exception(polling_response['error'])\n",
        "            time.sleep(2)\n",
        "\n",
        "        results = polling_response\n",
        "        full_text = results['text']\n",
        "        words = results['utterances']\n",
        "\n",
        "        # Format speaker-labeled transcript\n",
        "        speaker_transcript = \"\\n\".join(\n",
        "            f\"[{utt['start'] // 1000:.2f}s - {utt['end'] // 1000:.2f}s] Speaker {utt['speaker']} : {utt['text']}\"\n",
        "            for utt in words\n",
        "        )\n",
        "\n",
        "        latency = f\"{round(time.time() - start_time, 2)} sec\"\n",
        "        device_info = f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\"\n",
        "\n",
        "        # Optional GPU usage info\n",
        "        if torch.cuda.is_available():\n",
        "            subprocess.run([\"nvidia-smi\"])\n",
        "\n",
        "        return speaker_transcript, full_text, latency, device_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {str(e)}\", \"\", \"\", \"\"\n",
        "\n",
        "# 🎛️ Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🎙️ Speaker Diarization using Whisper (AssemblyAI Backend)\")\n",
        "    gr.Markdown(\"Upload audio file, transcribe and identify speakers using AssemblyAI.\")\n",
        "\n",
        "    audio_input = gr.Audio(type=\"filepath\", label=\"🎧 Upload Audio File\")\n",
        "    run_button = gr.Button(\"Run\")\n",
        "\n",
        "    diarized_output = gr.Textbox(label=\"🗣️ Speaker-Labeled Transcript\", lines=10)\n",
        "    full_transcript = gr.Textbox(label=\"📝 Full Transcript\", lines=6)\n",
        "    latency_info = gr.Textbox(label=\"⏱️ Latency\")\n",
        "    model_info = gr.Textbox(label=\"⚙️ Device Info\")\n",
        "\n",
        "    run_button.click(\n",
        "        fn=transcribe_and_diarize,\n",
        "        inputs=audio_input,\n",
        "        outputs=[diarized_output, full_transcript, latency_info, model_info]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T10:13:25.595743Z",
          "iopub.execute_input": "2025-06-06T10:13:25.596547Z",
          "iopub.status.idle": "2025-06-06T10:13:27.02487Z",
          "shell.execute_reply.started": "2025-06-06T10:13:25.59652Z",
          "shell.execute_reply": "2025-06-06T10:13:27.024318Z"
        },
        "id": "JKZssa8tQx5n",
        "outputId": "8c6d3460-64b6-400f-e0b0-a7c0f5ed1e4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "* Running on local URL:  http://127.0.0.1:7865\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://3140e57b45a1f5a64a.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://3140e57b45a1f5a64a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Fri Jun  6 10:13:52 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   39C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   40C    P8              9W /   70W |       3MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import torch\n",
        "import gradio as gr\n",
        "\n",
        "ASSEMBLYAI_API_KEY = None\n",
        "\n",
        "# Save API Key\n",
        "def save_api_key(user_key):\n",
        "    global ASSEMBLYAI_API_KEY\n",
        "    if not user_key or user_key.strip() == \"\":\n",
        "        return gr.update(value=\"❌ Please enter a valid API key.\"), gr.update(visible=False)\n",
        "    ASSEMBLYAI_API_KEY = user_key.strip()\n",
        "    return gr.update(value=\"✅ API key saved successfully! Upload audio to continue.\"), gr.update(visible=True)\n",
        "\n",
        "# Upload audio to AssemblyAI\n",
        "def upload_to_assemblyai(file_path):\n",
        "    headers = {'authorization': ASSEMBLYAI_API_KEY}\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = requests.post(\n",
        "            'https://api.assemblyai.com/v2/upload',\n",
        "            headers=headers,\n",
        "            files={'file': f}\n",
        "        )\n",
        "    return response.json()['upload_url']\n",
        "\n",
        "# Transcription with diarization\n",
        "def transcribe_and_diarize(audio_path):\n",
        "    global ASSEMBLYAI_API_KEY\n",
        "    try:\n",
        "        if not ASSEMBLYAI_API_KEY:\n",
        "            return \"[ERROR] API key not set.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        if not audio_path:\n",
        "            return \"[ERROR] No audio file provided.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "        upload_url = upload_to_assemblyai(audio_path)\n",
        "\n",
        "        headers = {\n",
        "            \"authorization\": ASSEMBLYAI_API_KEY,\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        request_data = {\n",
        "            \"audio_url\": upload_url,\n",
        "            \"speaker_labels\": True\n",
        "        }\n",
        "\n",
        "        response = requests.post(\"https://api.assemblyai.com/v2/transcript\", json=request_data, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            return f\"[ERROR] Transcript request failed: {response.text}\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        transcript_id = response.json()['id']\n",
        "        polling_url = f\"https://api.assemblyai.com/v2/transcript/{transcript_id}\"\n",
        "\n",
        "        while True:\n",
        "            polling_response = requests.get(polling_url, headers=headers)\n",
        "            if polling_response.status_code != 200:\n",
        "                return f\"[ERROR] Polling failed: {polling_response.text}\", \"\", \"\", \"\", \"\"\n",
        "            polling_json = polling_response.json()\n",
        "            if polling_json['status'] == 'completed':\n",
        "                break\n",
        "            elif polling_json['status'] == 'error':\n",
        "                raise Exception(polling_json['error'])\n",
        "            time.sleep(2)\n",
        "\n",
        "        results = polling_json\n",
        "        full_text = results['text']\n",
        "        words = results['utterances']\n",
        "\n",
        "        speaker_transcript = \"\\n\".join(\n",
        "            f\"[{utt['start'] // 1000:.2f}s - {utt['end'] // 1000:.2f}s] Speaker {utt['speaker']} : {utt['text']}\"\n",
        "            for utt in words\n",
        "        )\n",
        "\n",
        "        latency = f\"{round(time.time() - start_time, 2)} sec\"\n",
        "        device = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "        quality = \"✅ Diarization Quality: Good\\n- Multi-speaker support\\n- Word-level accuracy\"\n",
        "\n",
        "        return speaker_transcript, full_text, latency, device, quality\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {str(e)}\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "# Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🔐 AssemblyAI Speaker Diarization App\")\n",
        "\n",
        "    with gr.Row():\n",
        "        api_key_input = gr.Textbox(label=\"Enter AssemblyAI API Key\", type=\"password\")\n",
        "        submit_key_btn = gr.Button(\"Submit API Key\")\n",
        "    key_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    with gr.Column(visible=False) as main_app:\n",
        "        with gr.Row():\n",
        "            audio_input = gr.Audio(type=\"filepath\", label=\"🎧 Upload Audio File\")\n",
        "            run_button = gr.Button(\"Transcribe + Diarize\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=2):\n",
        "                diarized_output = gr.Textbox(label=\"🗣️ Speaker-Labeled Transcript\", lines=12)\n",
        "            with gr.Column(scale=2):\n",
        "                full_transcript = gr.Textbox(label=\"📄 Full Transcript\", lines=12)\n",
        "            with gr.Column(scale=1):\n",
        "                latency_info = gr.Textbox(label=\"⏱️ Latency\", interactive=False)\n",
        "                model_info = gr.Textbox(label=\"🖥️ GPU Info\", interactive=False)\n",
        "                quality_info = gr.Textbox(label=\"📌 Quality Notes\", interactive=False)\n",
        "\n",
        "    # Set the API key and show main UI\n",
        "    submit_key_btn.click(\n",
        "        fn=save_api_key,\n",
        "        inputs=[api_key_input],\n",
        "        outputs=[key_status, main_app]\n",
        "    )\n",
        "\n",
        "    # Run transcription\n",
        "    run_button.click(\n",
        "        fn=transcribe_and_diarize,\n",
        "        inputs=[audio_input],\n",
        "        outputs=[diarized_output, full_transcript, latency_info, model_info, quality_info]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-06T10:10:23.940579Z",
          "iopub.execute_input": "2025-06-06T10:10:23.94127Z",
          "iopub.status.idle": "2025-06-06T10:10:25.361758Z",
          "shell.execute_reply.started": "2025-06-06T10:10:23.941249Z",
          "shell.execute_reply": "2025-06-06T10:10:25.361125Z"
        },
        "id": "UvXb_Bm0Qx5o",
        "outputId": "8931e47d-e15e-4bbb-bb9e-b1159554f5f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "* Running on local URL:  http://127.0.0.1:7864\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://ddebe5fae88ba32946.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://ddebe5fae88ba32946.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper gradio"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-09T05:27:54.718489Z",
          "iopub.execute_input": "2025-06-09T05:27:54.719184Z",
          "iopub.status.idle": "2025-06-09T05:28:03.862792Z",
          "shell.execute_reply.started": "2025-06-09T05:27:54.719158Z",
          "shell.execute_reply": "2025-06-09T05:28:03.862082Z"
        },
        "id": "FpwVJtSTQx5p",
        "outputId": "599362d9-e6a1-452a-c4b1-93b5f92ca1c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\nCollecting gradio\n  Downloading gradio-5.33.0-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (1.26.4)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\nRequirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.6.0)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\nRequirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nCollecting fastapi<1.0,>=0.115.2 (from gradio)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.6.0-py3-none-any.whl.metadata (2.9 kB)\nCollecting gradio-client==1.10.2 (from gradio)\n  Downloading gradio_client-1.10.2-py3-none-any.whl.metadata (7.1 kB)\nCollecting groovy~=0.1 (from gradio)\n  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.18 (from gradio)\n  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nCollecting ruff>=0.9.3 (from gradio)\n  Downloading ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\nCollecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.47.0-py3-none-any.whl.metadata (6.2 kB)\nCollecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\nCollecting uvicorn>=0.14.0 (from gradio)\n  Downloading uvicorn-0.34.3-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (2025.3.2)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.2->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nCollecting starlette<1.0,>=0.40.0 (from gradio)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->openai-whisper) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->openai-whisper) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->openai-whisper) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->openai-whisper) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-5.33.0-py3-none-any.whl (54.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gradio_client-1.10.2-py3-none-any.whl (323 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\nDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\nDownloading ruff-0.11.13-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\nDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\nDownloading uvicorn-0.34.3-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ffmpy-0.6.0-py3-none-any.whl (5.5 kB)\nInstalling collected packages: uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, starlette, safehttpx, gradio-client, fastapi, gradio\nSuccessfully installed fastapi-0.115.12 ffmpy-0.6.0 gradio-5.33.0 gradio-client-1.10.2 groovy-0.1.2 python-multipart-0.0.20 ruff-0.11.13 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.3 uvicorn-0.34.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import torch\n",
        "import gradio as gr\n",
        "import whisper\n",
        "\n",
        "ASSEMBLYAI_API_KEY = None\n",
        "\n",
        "# Load Whisper large-v2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "whisper_model = whisper.load_model(\"small\", device=device)\n",
        "\n",
        "# Save API Key\n",
        "def save_api_key(user_key):\n",
        "    global ASSEMBLYAI_API_KEY\n",
        "    if not user_key or user_key.strip() == \"\":\n",
        "        return gr.update(value=\"❌ Please enter a valid API key.\"), gr.update(visible=False)\n",
        "    ASSEMBLYAI_API_KEY = user_key.strip()\n",
        "    return gr.update(value=\"✅ API key saved. Now upload your audio.\"), gr.update(visible=True)\n",
        "\n",
        "# Upload audio to AssemblyAI\n",
        "def upload_to_assemblyai(file_path):\n",
        "    headers = {'authorization': ASSEMBLYAI_API_KEY}\n",
        "    with open(file_path, 'rb') as f:\n",
        "        response = requests.post(\n",
        "            'https://api.assemblyai.com/v2/upload',\n",
        "            headers=headers,\n",
        "            files={'file': f}\n",
        "        )\n",
        "    return response.json()['upload_url']\n",
        "\n",
        "# Transcription + Diarization\n",
        "def transcribe_and_diarize(audio_path):\n",
        "    global ASSEMBLYAI_API_KEY\n",
        "    try:\n",
        "        if not ASSEMBLYAI_API_KEY:\n",
        "            return \"[ERROR] API key not set.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        if not audio_path:\n",
        "            return \"[ERROR] No audio provided.\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Transcribe using Whisper locally\n",
        "        whisper_result = whisper_model.transcribe(audio_path, verbose=False)\n",
        "        whisper_text = whisper_result['text']\n",
        "\n",
        "        # Upload to AssemblyAI\n",
        "        upload_url = upload_to_assemblyai(audio_path)\n",
        "\n",
        "        headers = {\n",
        "            \"authorization\": ASSEMBLYAI_API_KEY,\n",
        "            \"content-type\": \"application/json\"\n",
        "        }\n",
        "\n",
        "        request_data = {\n",
        "            \"audio_url\": upload_url,\n",
        "            \"speaker_labels\": True,\n",
        "            \"speaker_count\": 2  # ✅ Force diarization to detect only 2 speakers\n",
        "        }\n",
        "\n",
        "        response = requests.post(\"https://api.assemblyai.com/v2/transcript\", json=request_data, headers=headers)\n",
        "        if response.status_code != 200:\n",
        "            return f\"[ERROR] Transcript request failed: {response.text}\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "        transcript_id = response.json()['id']\n",
        "        polling_url = f\"https://api.assemblyai.com/v2/transcript/{transcript_id}\"\n",
        "\n",
        "        while True:\n",
        "            polling_response = requests.get(polling_url, headers=headers)\n",
        "            if polling_response.status_code != 200:\n",
        "                return f\"[ERROR] Polling failed: {polling_response.text}\", \"\", \"\", \"\", \"\"\n",
        "            polling_json = polling_response.json()\n",
        "            if polling_json['status'] == 'completed':\n",
        "                break\n",
        "            elif polling_json['status'] == 'error':\n",
        "                raise Exception(polling_json['error'])\n",
        "            time.sleep(2)\n",
        "\n",
        "        results = polling_json\n",
        "        full_text_assembly = results['text']\n",
        "        utterances = results['utterances']\n",
        "\n",
        "        speaker_output = \"\\n\".join(\n",
        "            f\"[{utt['start'] // 1000:.2f}s - {utt['end'] // 1000:.2f}s] Speaker {utt['speaker']} : {utt['text']}\"\n",
        "            for utt in utterances\n",
        "        )\n",
        "\n",
        "        latency = f\"{round(time.time() - start_time, 2)} sec\"\n",
        "        device_info = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\"\n",
        "        quality_info = \"✅ Diarization Quality: Good\\n- Powered by AssemblyAI\\n- Whisper transcription also included\"\n",
        "\n",
        "        return speaker_output, whisper_text, latency, device_info, quality_info\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"[ERROR] {str(e)}\", \"\", \"\", \"\", \"\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## 🧠 Whisper Large V2 + 🗣️ AssemblyAI Speaker Diarization\")\n",
        "    gr.Markdown(\"Enter API key, upload audio, and view diarized output using AssemblyAI and Whisper transcription.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        api_key_input = gr.Textbox(label=\"🔐 Enter AssemblyAI API Key\", type=\"password\")\n",
        "        submit_key_btn = gr.Button(\"✅ Submit API Key\")\n",
        "    key_status = gr.Textbox(label=\"Status\", interactive=False)\n",
        "\n",
        "    with gr.Column(visible=False) as main_app:\n",
        "        audio_input = gr.Audio(type=\"filepath\", label=\"🎧 Upload Audio File\")\n",
        "        run_button = gr.Button(\"🚀 Transcribe + Diarize\")\n",
        "\n",
        "        with gr.Row():\n",
        "            diarized_output = gr.Textbox(label=\"🗣️ Speaker-Labeled Transcript (AssemblyAI)\", lines=12)\n",
        "            full_transcript = gr.Textbox(label=\"📝 Whisper Full Transcript (Local)\", lines=12)\n",
        "\n",
        "        with gr.Row():\n",
        "            latency_info = gr.Textbox(label=\"⏱️ Latency\", interactive=False)\n",
        "            model_info = gr.Textbox(label=\"🖥️ GPU Info\", interactive=False)\n",
        "            quality_info = gr.Textbox(label=\"📌 Quality Notes\", interactive=False)\n",
        "\n",
        "    # Save API key\n",
        "    submit_key_btn.click(\n",
        "        fn=save_api_key,\n",
        "        inputs=[api_key_input],\n",
        "        outputs=[key_status, main_app]\n",
        "    )\n",
        "\n",
        "    # Run transcription + diarization\n",
        "    run_button.click(\n",
        "        fn=transcribe_and_diarize,\n",
        "        inputs=[audio_input],\n",
        "        outputs=[diarized_output, full_transcript, latency_info, model_info, quality_info]\n",
        "    )\n",
        "\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-09T05:31:11.071886Z",
          "iopub.execute_input": "2025-06-09T05:31:11.072222Z",
          "iopub.status.idle": "2025-06-09T05:31:16.238872Z",
          "shell.execute_reply.started": "2025-06-09T05:31:11.072195Z",
          "shell.execute_reply": "2025-06-09T05:31:16.238313Z"
        },
        "id": "NWpH9Hx9Qx5q",
        "outputId": "334652ac-98b0-4d56-8bc4-7619c4b08970"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "* Running on local URL:  http://127.0.0.1:7861\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://141249567aa2272e05.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "<div><iframe src=\"https://141249567aa2272e05.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
          },
          "metadata": {}
        },
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": ""
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Detected language: English\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 6566/6566 [00:03<00:00, 1990.13frames/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "tJdVJhbYQx5r"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}